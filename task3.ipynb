{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgACr9Jss8Uc",
        "colab_type": "text"
      },
      "source": [
        "# **Task 3. Neural classification **\n",
        "Comparison of  soft- hard- and neural algorithms\n",
        "As a target metric for estimation I've chosen AUC-ROC score, because it is basically one of the classic measurements saying how good our model is when distinguishing classes. So, the model with a higher meaning will be considered to be the best one.\n",
        "\n",
        "The models we are going to constrauct are the following:\n",
        "1. Fasttext model with the score of 91,7%\n",
        "2. \"Hard baseline\" of TFIDF + XGBoost. Inn this particular version it didn't finish calculations due to the end of collab time. But the only time it performed it gave me around 88%, which is a lower result. Taking into consideration time it takes.... well I have no idea how to use it with really huge samples\n",
        "3. Real NN - still in process... and I have no idea how much time it can take\n",
        "\n",
        "Small spoiler: really big models do not run because of the limited computing power i have (even usinng Google collab - 8h not enough). But still doestn't mean that they wouldn't work!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWAQtbIN24y",
        "colab_type": "code",
        "outputId": "18fad8d7-41e2-41a2-bcc2-cca5446229c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "! pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3016225 sha256=16b8f479670f261916dc0b03c5898def5faa50fa52023833b15ff0d0a3596564\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDI6bjo3Dckz",
        "colab_type": "text"
      },
      "source": [
        "# **3.1 Fasttext classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1vaZvydOOGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import fasttext\n",
        "import bz2\n",
        "import csv\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0f3itiObRJ",
        "colab_type": "code",
        "outputId": "71aef1db-055a-42fd-e172-588f368f40bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train=bz2.BZ2File(\"train.ft.txt.bz2\")\n",
        "train=train.readlines()\n",
        "train=[x.decode('utf-8') for x in train]\n",
        "print(len(train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfIJth2_PHy-",
        "colab_type": "code",
        "outputId": "31127ad2-78b3-4be0-86c1-90213c4b4b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test=bz2.BZ2File(\"test.ft.txt.bz2\")\n",
        "test=test.readlines()\n",
        "test=[x.decode('utf-8') for x in test]\n",
        "print(len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaFTgA0TTl5u",
        "colab_type": "code",
        "outputId": "52a48aad-41a6-442f-e4e3-02bd62de1750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#we prepare our training sample\n",
        "train = pd.DataFrame(train)\n",
        "train.to_csv(\"train.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "# after, we build a model itself\n",
        "model1 = fasttext.train_supervised('train.txt',label_prefix='__label__', thread=4, epoch = 10)\n",
        "print(model1.labels, 'the labels')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__label__1', '__label__2'] the labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyWDTK3nbgoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now we prepare the test data\n",
        "# firstly, we have to get rid of labels from the sample  \n",
        "testnew = [w.replace('__label__2 ', '') for w in test]\n",
        "testnew = [w.replace('__label__1 ', '') for w in testnew]\n",
        "testnew = [w.replace('\\n', '') for w in testnew]\n",
        "\n",
        "# Use the predict function \n",
        "pred = model1.predict(testnew)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUvdijd9geae",
        "colab_type": "code",
        "outputId": "fbc4790e-4ebe-4ab8-efdc-b4d14fe5630c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# he we channge the labels from test sample and the model predictions ones and zeros\n",
        "labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\n",
        "pred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]\n",
        "\n",
        "# we use the roc auc curve as a measurement because it demonstrates how good our model is \n",
        "print(roc_auc_score(labels, pred_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9173650000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PjWfwOCkBJt",
        "colab_type": "text"
      },
      "source": [
        "We got a result of 91,7% \n",
        "Will compare this to hardbaseline model soon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7sm1-5IDmNQ",
        "colab_type": "text"
      },
      "source": [
        "# **3.2 TFIDF + XGBoost classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sb8Ja4ckoHi",
        "colab_type": "code",
        "outputId": "8b7c0a79-26a9-4b1b-ff64-ad5d7a78ef0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JQ5zHp796gs",
        "colab_type": "code",
        "outputId": "8b93d2cc-339e-43db-bd14-468ac058ec55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "! pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.3)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3016215 sha256=70bb3d25b824c112f7c08d990a94fbddf9f25a82274e0be1230827296bc5df24\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLL7kXbVj_ao",
        "colab_type": "code",
        "outputId": "6fd6cf43-1035-4a1c-8c63-29a9285cc9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import fasttext\n",
        "import string\n",
        "import re\n",
        "import bz2\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Embedding, SpatialDropout1D\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6tTimuS9f0i",
        "colab_type": "code",
        "outputId": "dd7141c3-205a-4f43-b28c-53e1639553ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train=bz2.BZ2File(\"train.ft.txt.bz2\")\n",
        "train=train.readlines()\n",
        "train=[x.decode('utf-8') for x in train]\n",
        "print(len(train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3600000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8HgMrqq9jB7",
        "colab_type": "code",
        "outputId": "4bf88065-5bec-4751-8606-d138bffcce7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test=bz2.BZ2File(\"test.ft.txt.bz2\")\n",
        "test=test.readlines()\n",
        "test=[x.decode('utf-8') for x in test]\n",
        "print(len(test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwqlzEbkeCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_crNEyJOkkkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Tokenizerr(str_input):\n",
        "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
        "    porter_stemmer=nltk.PorterStemmer()\n",
        "    words = [porter_stemmer.stem(word) for word in words]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMPwuXHilZOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=Tokenizerr, stop_words=stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-EYcryXWpZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YSb8PxK9s0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we prepare our training sample\n",
        "train = pd.DataFrame(train)\n",
        "train.to_csv(\"train.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jUZt-Olldy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "X_train = []\n",
        "for i in f: \n",
        "    X_train.append(i[11:])\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCaa7IMZlkQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "y_train = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_train.append(0)\n",
        "    else:\n",
        "        y_train.append(1)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u9DpVImpf0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.DataFrame(test)\n",
        "test.to_csv(\"test.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOPAHBoup75R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "X_test = []\n",
        "for i in f: \n",
        "    X_test.append(i[11:])\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-iftZTSqOZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "y_test = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_test.append(0)\n",
        "    else:\n",
        "        y_test.append(1)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X60AbCyqVPr",
        "colab_type": "code",
        "outputId": "698d0421-dee5-4f0e-f1b2-ebcf1aa98ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "train = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxTotPzEQqji",
        "colab_type": "text"
      },
      "source": [
        "Well, we have a warning here that some of the tokens generated are not apparently reflect the appropriate lannguage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTtCBt0yqdZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGCgUxLoyeqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_model = XGBClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUfTjJvsyfZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_model.fit(train, y_train)\n",
        "\n",
        "XGB_model.save_model(\"XGB_model.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvNlxWGUypB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_model.load_model(\"XGB_model.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQo5ZpwysE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_pred = XGB_model.predict(test)\n",
        "\n",
        "with open('XGB_pred', 'wb') as f:\n",
        "     pickle.dump(XGB_pred, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOIRdnpPysfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('XGB_pred', 'rb') as f:\n",
        "     XGB_pred = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoPTf7X1yx_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XGB_auc = roc_auc_score(y_test, XGB_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqqhR-2Vy0fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"SCORE:\", XGB_auc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCrzlwZLOjr",
        "colab_type": "text"
      },
      "source": [
        "Here is the performance of our XGBoost..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEwx_MBCYmqa",
        "colab_type": "text"
      },
      "source": [
        "Actually, I've spent days running and running this stuff again. And every single time the process was interrupted. It really sucks. The one time I got a result - it was a bit lower than 88%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZW01iw_CPRk",
        "colab_type": "text"
      },
      "source": [
        "## **3.3 NN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvzU185iKy-O",
        "colab_type": "code",
        "outputId": "79c34ec9-2880-4bab-b972-d2567bfaf252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAdbDfF26a2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkHOC4yAOfXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVWm21u_CQlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=bz2.BZ2File(\"train.ft.txt.bz2\")\n",
        "train=train.readlines()\n",
        "train=[x.decode('utf-8') for x in train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxOFhEatNrvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test=bz2.BZ2File(\"test.ft.txt.bz2\")\n",
        "test=test.readlines()\n",
        "test=[x.decode('utf-8') for x in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL3nCbEZNvVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we prepare our training sample\n",
        "train = pd.DataFrame(train)\n",
        "train.to_csv(\"train.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GTiFWsGOilf",
        "colab_type": "text"
      },
      "source": [
        "As previously, we transform it to fit in the model than "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSMpnbHjN9C6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "X_train = []\n",
        "for i in f: \n",
        "    X_train.append(i[11:])\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQdVbRKtN9vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('train.txt', 'r', encoding ='utf-8')\n",
        "y_train = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_train.append(0)\n",
        "    else:\n",
        "        y_train.append(1)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcsHJj-GOAeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.DataFrame(test)\n",
        "test.to_csv(\"test.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXLWRH2GOELw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "X_test = []\n",
        "for i in f: \n",
        "    X_test.append(i[11:])\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCybauwPOG67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('test.txt', 'r', encoding ='utf-8')\n",
        "y_test = []\n",
        "for i in f: \n",
        "    if i[:10] == '__label__1':\n",
        "        y_test.append(0)\n",
        "    else:\n",
        "        y_test.append(1)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DtTcTap706z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI9TNkOHNm06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AccCHLuHMOTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#padding\n",
        "max_words = 500\n",
        "max_len = 150\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(train)\n",
        "sequences = tok.texts_to_sequences(train)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGNVxQXMRBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we define a function for building a recurrent NN \n",
        "def recNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model3 = Model(inputs=inputs,outputs=layer)\n",
        "    return model3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V2mQpAvMZZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3 = recNN()\n",
        "model3.summary()\n",
        "model3.compile(loss='binary_crossentropy',optimizer=\"sgd\",metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl9WelAMMq1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the data to RNN with batch size of 128 and 5 epochs\n",
        "model.fit(sequences_matrix,y_train,batch_size=128,epochs=5, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOfE78MnMudi",
        "colab_type": "text"
      },
      "source": [
        "Here, in an ideal world, we get a AUC ROC score and try to compare it with other results. But realistically, I've spend several nights trying to figure out how to make it work. No chance.\n",
        "Any advice on how to do it? Theoretically, it will work on more powerfull machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11UHSqEPnLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}